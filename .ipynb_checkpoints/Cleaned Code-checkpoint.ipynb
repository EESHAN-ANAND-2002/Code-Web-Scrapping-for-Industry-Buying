{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02284325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import sys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57792d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function finds all the category which can be used for collecting the datas\n",
    "def get_the_category_urls(url_having_all_categories):\n",
    "    #validate whether the url is present or not\n",
    "    if not validators.url(url_having_all_categories):\n",
    "        return None, 1\n",
    "    url = url_having_all_categories\n",
    "    max_retries = 10\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status() # Raises HTTPError for bad responses\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#             print(\"Title:\", soup.title.text)\n",
    "            break\n",
    "        except requests.RequestException as e:\n",
    "#             print(f\"Attempt {attempt}: Request failed - {e}\")\n",
    "            pass\n",
    "\n",
    "    else:\n",
    "#         print(f\"Failed to retrieve the content after {max_retries} attempts.\")\n",
    "        return None, 1\n",
    "\n",
    "    \n",
    "    all_hrefs = [a.get('href') for a in soup.find_all('a', class_='catHead')]\n",
    "    all_hrefs=  [url[:-12]+i for i in all_hrefs]\n",
    "    \n",
    "    return all_hrefs, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1764671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what this function do is go inside all the category url and finds all the product's url\n",
    "#we return three thing\n",
    "#1.list_subcategories->having the urls for the product\n",
    "#2.error_logs- lets say if accessing the given_url is done, we can find all the pages and subcategories which failed to load\n",
    "#3.error-if the website fails to load, we can simply return the message 1\n",
    "def find_all_urls(given_url):\n",
    "    error_logs=[]\n",
    "    list_subcategories=[]\n",
    "    url_k=given_url\n",
    "\n",
    "\n",
    "    # url = \"https://www.hippostores.com/ihbcategory/sanitary-ware-and-bath-fittings-sbf\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    \n",
    "    #validate whether the url exist or not\n",
    "    if not validators.url(url_k):\n",
    "        return list_subcategories, error_logs, 1\n",
    "    \n",
    "    #prevent from not getting a responce and getting a bad responce\n",
    "    maxtries=5\n",
    "    error=1\n",
    "    for tries in range(maxtries):\n",
    "        \n",
    "        session = requests.Session()\n",
    "        response = session.get(url_k, headers=headers)\n",
    "        if response.status_code == 200 and response.status_code != 400:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            error=0\n",
    "            break\n",
    "\n",
    "    if error==1:\n",
    "        return list_subcategories, error_logs, 1\n",
    "\n",
    "    #list_category contains all the links for the subcategories in the specific category\n",
    "    list_category=[]\n",
    "    cat_colm_elements = soup.find_all(class_=\"cat-colm\")\n",
    "\n",
    "\n",
    "    # Extract and print the first link from each element\n",
    "    for cat_colm in cat_colm_elements:\n",
    "        href_link = \"https://www.industrybuying.com\"+cat_colm.find('a')['href']\n",
    "        list_category.append(href_link)\n",
    "\n",
    "    #so we get our list category which contains all the links for the subcategories in the specific category\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for i in range(len(list_category)):\n",
    "        error_logs.append([])\n",
    "        #this contains all the urls of that specific subcategory\n",
    "        list_of_url_individual=[]\n",
    "\n",
    "        #link_one_category is specific url of a subcategory\n",
    "        link_one_category=list_category[i]\n",
    "        error_logs[-1].append(link_one_category)\n",
    "        headers = {\n",
    "          'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        \n",
    "        maxtries=5\n",
    "        error=1\n",
    "        for tries in range(maxtries):\n",
    "            session = requests.Session()\n",
    "            response = session.get(link_one_category, headers=headers)\n",
    "\n",
    "            #prevent from not getting a responce and getting a bad responce\n",
    "            if response.status_code == 200 and response.status_code != 400:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                error=0\n",
    "                error_logs[-1].append(0) #means this category is successfully captured\n",
    "                error_logs[-1].append([])\n",
    "                break\n",
    "        if error==1:\n",
    "            error_logs[-1].append(1,[], None, None)\n",
    "            continue\n",
    "\n",
    "        span_element = soup.find('span', class_='productslimit')\n",
    "\n",
    "        span_text = span_element.get_text()\n",
    "\n",
    "        span_text_parts = span_text.strip('()').split()\n",
    "\n",
    "        products_one_page = int(span_text_parts[0].split('-')[1])\n",
    "        total_products = int(span_text_parts[-1])\n",
    "        pages=math.ceil(total_products / products_one_page)\n",
    "\n",
    "        #now all we have to do is go page to page and pick all the url having the links\n",
    "        for page in range(pages):\n",
    "            url_page_wise=link_one_category+ f\"?page={page+1}\"\n",
    "\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "            }\n",
    "            \n",
    "            maxtries=5\n",
    "            error=1\n",
    "            for tries in range(maxtries):\n",
    "                session = requests.Session()\n",
    "                response = session.get(url_page_wise, headers=headers)\n",
    "                #prevent from not getting a responce and getting a bad responce\n",
    "                if response.status_code == 200 and response.status_code != 400:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    error=0\n",
    "                    break\n",
    "                if error==1:\n",
    "                    error_logs[-1][-1].append(page)\n",
    "                    continue\n",
    "            \n",
    "                \n",
    "\n",
    "            div_elements = soup.find_all('div', class_='AH_ProductView')\n",
    "\n",
    "\n",
    "\n",
    "            # Loop through each div element\n",
    "            for div_element in div_elements:\n",
    "                # Find the first anchor element within the div\n",
    "                first_link = div_element.find('a')\n",
    "\n",
    "                # If a link is found, append its href to the list\n",
    "                if first_link:\n",
    "                    href_link = first_link.get('href')\n",
    "                    list_of_url_individual.append(href_link)\n",
    "        \n",
    "        error_logs[-1].append(len(list_of_url_individual))\n",
    "        list_subcategories.append(list_of_url_individual)\n",
    "        error_logs[-1].append(len(list(set(list_of_url_individual))))\n",
    "#         print(i, end=\" \")\n",
    "    \n",
    "    #remove duplicate from the list_subcategories\n",
    "    list_subcategories_copy=[]\n",
    "    for lis in list_subcategories:\n",
    "        lis_set=list(set(lis))\n",
    "        list_subcategories_copy.append(lis_set)\n",
    "    \n",
    "    list_subcategories=list_subcategories_copy\n",
    "    #we can return this table without any duplicate index\n",
    "    \n",
    "    \n",
    "    return list_subcategories, error_logs, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc573055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logs_table(logs, index):\n",
    "    for log in logs:\n",
    "        log[2] = [str(num) for num in log[2]]\n",
    "        lis=log[2]\n",
    "        log[2]=', '.join(lis)\n",
    "    df=pd.DataFrame(logs, columns=['URL','Subcategory Status','Page Not Retrieved','Total Product','Non duplicated Products'])\n",
    "    df.to_csv(f\"logs{index}.csv\", index=False)\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea88ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what it does is find all the prices inside the table listing the mrp and other stuff in each product\n",
    "def find_prices(soup):\n",
    "    mrp=None\n",
    "    discount=None\n",
    "    discount_flag=None\n",
    "    \n",
    "    price_text = soup.find('span', class_='mainPrice')\n",
    "    \n",
    "    \n",
    "    if price_text:\n",
    "        price_text=price_text.text\n",
    "        price_integer = int(price_text.replace('Rs.', '').replace(',', ''))\n",
    "        mrp=price_integer\n",
    "        discount_flag=0\n",
    "        discount=\"1-\"+str(mrp)\n",
    "        return mrp, discount, discount_flag, 0\n",
    "    \n",
    "    mrp_listing = soup.find('del', {'id': 'AH_ListPrice'})\n",
    "    bulk_table= soup.find('div', {'class': 'ah-bulk-qty-table'})\n",
    "    per_piece_price=soup.find('span', class_='AH_PricePerPiece')\n",
    "    \n",
    "    if mrp_listing is None and bulk_table is None and per_piece_price is None:\n",
    "        return None, None, None, 1\n",
    "\n",
    "    \n",
    "    \n",
    "    #find a condition with no discount on individual as well as on packs\n",
    "    if mrp_listing is None and bulk_table is None:\n",
    "        discount_flag=0\n",
    "        #now since there wont be any discount either ways we can have our discount as:- mrp-1\n",
    "        mrp=per_piece_price.get_text(strip=True).replace(\",\", \"\")\n",
    "        discount=\"1-\"+str(mrp)\n",
    "        \n",
    "    \n",
    "    #find a condition with discount only on 1 item \n",
    "    elif mrp_listing and bulk_table is None:\n",
    "        discount_flag=1\n",
    "        #now my discount price and my mrp would be different\n",
    "        discount=per_piece_price.get_text(strip=True).replace(\",\", \"\")\n",
    "        discount=\"1-\"+str(discount)\n",
    "        mrp=mrp_listing.get_text(strip=True).replace(\",\", \"\")\n",
    "        \n",
    "        \n",
    "    #find a condition where discount is only present on bulk\n",
    "    elif mrp_listing is None and bulk_table:\n",
    "        discount_flag=2\n",
    "        #now since there wont be any discount(individual) we can have our discount initially as:- mrp-1\n",
    "        mrp=per_piece_price.get_text(strip=True).replace(\",\", \"\")\n",
    "        discount=\"1-\"+str(mrp)\n",
    "        \n",
    "        element_texts = []\n",
    "\n",
    "        # Start the index at 0\n",
    "        index = 0\n",
    "\n",
    "        while True:\n",
    "            # Generate the class name with the current index\n",
    "            class_name = f'table-row bulk-option AH_BulkPriceOption AH_BulkPriceOption_{index}'\n",
    "\n",
    "            # Find all div elements with the specific class\n",
    "            elements = soup.find_all('div', class_=class_name)\n",
    "\n",
    "            # If elements are found, proceed\n",
    "            if elements:\n",
    "                for element in elements:\n",
    "                    # Find all div elements with class \"table-cell\" inside the current element\n",
    "                    table_cells = element.find_all('div', class_='table-cell')\n",
    "\n",
    "                    # Extract text from each \"table-cell\" and add it to the list\n",
    "                    cell_texts = [cell.get_text(strip=True) for cell in table_cells]\n",
    "                    element_texts.append(cell_texts)\n",
    "\n",
    "                # Increment the index for the next iteration\n",
    "                index += 1\n",
    "            else:\n",
    "                # If no elements are found, break the loop\n",
    "                break\n",
    "\n",
    "        # Print the list of texts for each element\n",
    "        for each_element in element_texts:\n",
    "            a=each_element[1]\n",
    "            b=each_element[2]\n",
    "            a=a.split('-')[0].strip().split('+')[0]\n",
    "            b=b.split(\" \")[-1]\n",
    "            #now we got these dudes, we can simple append this to string discount\n",
    "            discount+=\"; \"+a+\"-\"+b\n",
    "        \n",
    "    \n",
    "    #find a condition where the discount is present on both, individual and bulk\n",
    "    else:\n",
    "        discount_flag=3\n",
    "        #now we can different discount for individual and bulk\n",
    "        discount=per_piece_price.get_text(strip=True).replace(\",\", \"\")\n",
    "        discount=\"1-\"+str(discount)\n",
    "        mrp=mrp_listing.get_text(strip=True).replace(\",\", \"\") \n",
    "        \n",
    "        element_texts = []\n",
    "\n",
    "        # Start the index at 0\n",
    "        index = 0\n",
    "\n",
    "        while True:\n",
    "            # Generate the class name with the current index\n",
    "            class_name = f'table-row bulk-option AH_BulkPriceOption AH_BulkPriceOption_{index}'\n",
    "\n",
    "            # Find all div elements with the specific class\n",
    "            elements = soup.find_all('div', class_=class_name)\n",
    "\n",
    "            # If elements are found, proceed\n",
    "            if elements:\n",
    "                for element in elements:\n",
    "                    # Find all div elements with class \"table-cell\" inside the current element\n",
    "                    table_cells = element.find_all('div', class_='table-cell')\n",
    "\n",
    "                    # Extract text from each \"table-cell\" and add it to the list\n",
    "                    cell_texts = [cell.get_text(strip=True) for cell in table_cells]\n",
    "                    element_texts.append(cell_texts)\n",
    "\n",
    "                # Increment the index for the next iteration\n",
    "                index += 1\n",
    "            else:\n",
    "                # If no elements are found, break the loop\n",
    "                break\n",
    "\n",
    "        # Print the list of texts for each element\n",
    "        for each_element in element_texts:\n",
    "            a=each_element[1]\n",
    "            b=each_element[2]\n",
    "            a=a.split('-')[0].strip().split('+')[0]\n",
    "            b=b.split(\" \")[-1]\n",
    "            #now we got these dudes, we can simple append this to string discount\n",
    "            discount+=\"; \"+a+\"-\"+b\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #our discount will be in integer but our mrp must be integer\n",
    "    mrp=int(mrp)\n",
    "    \n",
    "    \n",
    "    return mrp, discount, discount_flag, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b4d013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is an important match\n",
      "Today is an important match\n",
      "Today is an important match\n",
      "Health Health Philips bulbs\n"
     ]
    }
   ],
   "source": [
    "#these function can be used for cleaning\n",
    "#for eg:-name of the product is:-Super Bulb with ultra shining(332743), then my name_cleaner outputs:-Super Bulb with ultra shining\n",
    "def name_cleaner(str):\n",
    "  l=str.split(' ')\n",
    "  clean_str=\"\"\n",
    "  if l[-1].isnumeric():\n",
    "    clean_str=' '.join(l[0:-2])\n",
    "  elif l[-1][1:-1].isnumeric():\n",
    "    clean_str=' '.join(l[0:-1])\n",
    "  else:\n",
    "    clean_str=str\n",
    "\n",
    "  return clean_str\n",
    "\n",
    "print(name_cleaner(\"Today is an important match - 201920\"))\n",
    "print(name_cleaner(\"Today is an important match (38929)\"))\n",
    "print(name_cleaner(\"Today is an important match\"))\n",
    "\n",
    "#string star remover and capitalizer\n",
    "def process_string(input_string):\n",
    "    cleaned_string = input_string.rstrip('.*').lower().capitalize()\n",
    "    return cleaned_string\n",
    "\n",
    "print(process_string(\"health.\"), process_string(\"health*\"), process_string(\"Philips bulbs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4498821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#once we send our both list through which we can make the dataset\n",
    "def get_dataset(list_mandatory, list_optional, set_with_columns, index):\n",
    "    columns = [\"Primary Key\", \"Brand\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\", \"Product\",\n",
    "               \"Name\", \"Piece Quantity\", \"MRP\", \"Discount Price(Max)\", \"Discount Price(Min)\",\n",
    "               \"Discount Check Flag\", \"URL\"]\n",
    "\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df_mandatory = pd.DataFrame(list_mandatory, columns=columns)   \n",
    "    \n",
    "    df_mandatory.to_csv(f\"df{index}_mandatory.csv\",index=False)\n",
    "    #we have made the mandatory dataset\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    # Iterate through each row in list_optional\n",
    "    for row in list_optional:\n",
    "        # Create a dictionary to store values for the current row\n",
    "        row_values = {column: None for column in set_with_columns}\n",
    "\n",
    "        # Update the values for the columns present in the current row\n",
    "        for column, value in row:\n",
    "            row_values[column] = value\n",
    "\n",
    "        # Append the row to the list\n",
    "        data.append(row_values)\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df_optional = pd.DataFrame(data)\n",
    "    df_optional.to_csv(f\"df{index}_optional.csv\", index=False)\n",
    "    #we have made the optional dataset\n",
    "    df=df_optional.copy()\n",
    "    # Sort columns based on the number of NaN values in each column\n",
    "    sorted_columns = df_optional.isna().sum().sort_values().index\n",
    "    df = df[sorted_columns]\n",
    "\n",
    "    df.to_csv(f\"df{index}_optional.csv\", index=False)\n",
    "    \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e39643dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collect(list_subcategories, index):\n",
    "    list_mandatory=[]\n",
    "    product_with_inconsistency=[]\n",
    "    set_with_columns=set()\n",
    "    list_optional=[]\n",
    "    for i in range(len(list_subcategories)):\n",
    "        for j in range(len(list_subcategories[i])):\n",
    "    #         url=\"https://www.industrybuying.com/knapsack-sprayer-agripro-AGR.KNA.45809501/\"\n",
    "\n",
    "            url=\"https://www.industrybuying.com\"+list_subcategories[i][j]\n",
    "\n",
    "\n",
    "            options = Options()\n",
    "            options.add_argument('--headless')\n",
    "\n",
    "            try:\n",
    "                # Attempt to open the Chrome WebDriver\n",
    "                driver = webdriver.Chrome(options=options)\n",
    "\n",
    "                # Attempt to navigate to the given URL\n",
    "                driver.get(url)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle the exception (e.g., print an error message)\n",
    "                product_with_inconsistency.append([url, \"URL inconsistency\"])\n",
    "                # Optionally, you may want to quit the driver if an exception occurs\n",
    "                driver.quit()\n",
    "#                 print(f\"**->{len(product_with_inconsistency)}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            try:\n",
    "                button = WebDriverWait(driver, 1).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"main\"]/div/div/div/div/div/div[1]/div[4]/div[2]/div[7]/div[1]/table/tbody/tr[8]/td/a')))\n",
    "                if button:\n",
    "                    button.click()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "\n",
    "            # time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                # Get the latest HTML content\n",
    "                html_content = driver.page_source\n",
    "            except Exception as e:\n",
    "                product_with_inconsistency.append([url, \"URL inconsistency\"])\n",
    "                driver.quit()\n",
    "#                 print(f\"**->{len(product_with_inconsistency)}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Use BeautifulSoup to parse the HTML content\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "            #first we have to get the mandotory fields\n",
    "            #it contains:- Primary Key, Brand, Category, Sub Category, Hidden Categories, Product, Name\n",
    "            #MRP, Discount Price, Discount Check Flag, URL\n",
    "\n",
    "\n",
    "            #lets figure out the prices and the discounts and discount flag\n",
    "            mrp, discount, discount_flag, error=find_prices(soup)\n",
    "\n",
    "            if error==1:\n",
    "                product_with_inconsistency.append([url, \"Bad Gateway inconsistency\"])\n",
    "#                 print(f\"Bad Gateway ->{len(product_with_inconsistency)}\")\n",
    "                continue\n",
    "\n",
    "            discount_max=discount.split(\";\")[0].split(\"-\")[1]\n",
    "            discount_min=discount.split(\"; \")[-1].split(\"-\")[-1]\n",
    "\n",
    "\n",
    "            #lets get the brand\n",
    "            h2_element = soup.find('h2', class_='by')\n",
    "\n",
    "            if h2_element:\n",
    "                a_tag = h2_element.find('a')\n",
    "\n",
    "                if a_tag:\n",
    "                    brand=a_tag.text.strip()\n",
    "\n",
    "\n",
    "            #now we get the category, subcategory, name and the product\n",
    "            links = soup.select('div.commonBreadCrums a')\n",
    "\n",
    "            # Extract the text inside each link and store it in a list\n",
    "            links_list = [link.text.strip() for link in links]\n",
    "\n",
    "            #if we found out the length of the links_list is less than 4, we will continue\n",
    "            if len(links_list)<3:\n",
    "                product_with_inconsistency.append([url, \"Category listing inconsistency\"])\n",
    "#                 print(f\"**->{len(product_with_inconsistency)}\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            #we have got all the subcategory using this\n",
    "            l=links_list.copy()\n",
    "            dict_cat={}\n",
    "            for z in range(6):\n",
    "                if len(l)-2>z and z!=5:\n",
    "                    dict_cat[f\"l{z+1}\"]=l[z+1]\n",
    "                else:\n",
    "                    dict_cat[f\"l{z+1}\"]=\"\"\n",
    "\n",
    "                if z==5 and len(l)-2>z:\n",
    "                    dict_cat[f\"l{z+1}\"]=\" \".join([v for v in l[z+1:-1]])       \n",
    "    \n",
    "            product=l[-1]\n",
    "\n",
    "            #now we will have some constants\n",
    "            piece_quantity=1\n",
    "\n",
    "\n",
    "            #now we have to get the name\n",
    "            name = soup.find('span', class_='productTitle').find('h1').text.strip()\n",
    "\n",
    "\n",
    "            #first of all we would be scrapping is optional stuff\n",
    "            dict_table={}\n",
    "\n",
    "            table_element = soup.find('div', {'class': 'tabDetailsContainer', 'id': 'famSpec'})\n",
    "\n",
    "\n",
    "            # Find all div elements with class 'filterRow'\n",
    "            if table_element:\n",
    "                filter_rows = table_element.find_all('div', class_='filterRow')\n",
    "            else:\n",
    "                filter_rows=None\n",
    "            # Initialize an empty list to store the tuple pairs\n",
    "            dict_table={}\n",
    "\n",
    "\n",
    "\n",
    "            # Loop through each filter row and extract feature name and value\n",
    "            if filter_rows:\n",
    "                for row in filter_rows:\n",
    "                    key = row.find('div', class_='featureNamePr').text.strip()\n",
    "                    key=process_string(key)\n",
    "                    value = row.find('div', class_='featureValuePr').text.replace(':', '').strip()\n",
    "                    #now we have got the feature_name and feature_value\n",
    "                    #we can put this in a dictionary\n",
    "                    dict_table[key]=str(value)\n",
    "\n",
    "\n",
    "            #lets consider a scenario, if the model no is not present, we wont be definitely moving forward \n",
    "            u_id=None\n",
    "            if 'Model no' in dict_table:\n",
    "                u_id=dict_table['Model no']\n",
    "            else:\n",
    "                #if model_no not present, then put the id format as:-ibyymmddhhmmssuuuuuu (u denotes microseconds)\n",
    "                current_time = datetime.now()\n",
    "                # Format the time to \"yymmddhhmmssuuuuuu\"\n",
    "                formatted_time = current_time.strftime(\"%y%m%d%H%M%S%f\")[:]  \n",
    "                u_id=\"ib\"+formatted_time\n",
    "\n",
    "            if u_id =='-':\n",
    "                #if model_no not present, then put the id format as:-ibyymmddhhmmssuuuuuu (u denotes microseconds)\n",
    "                current_time = datetime.now()\n",
    "                # Format the time to \"yymmddhhmmssuuuuuu\"\n",
    "                formatted_time = current_time.strftime(\"%y%m%d%H%M%S%f\")[:]  \n",
    "                u_id=\"ib\"+formatted_time\n",
    "                \n",
    "            dict_table['Model no']=u_id \n",
    "\n",
    "            #now we can add all the optional columns in the set\n",
    "            for k in dict_table:\n",
    "                set_with_columns.add(k)\n",
    "\n",
    "\n",
    "            key_value_pairs_as_lists = [[key, value] for key, value in dict_table.items()]\n",
    "            list_optional.append(key_value_pairs_as_lists)\n",
    "            print(i*2+j+1)\n",
    "\n",
    "    #         print(brand)\n",
    "\n",
    "    #         print(category, subcategory, product, name, sep='\\t')\n",
    "\n",
    "    #         print(u_id, discount_flag, mrp, discount_max, discount_min, sep='\\t')\n",
    "\n",
    "            #Primary Key, Brand, Category, Sub Category, Hidden Categories, Product, Name, Piece Quantity, MRP, Discount Price(Max), Discount Price(Min), Discount Check Flag, URL\n",
    "            list_mandatory.append([u_id, brand, dict_cat[\"l1\"], dict_cat[\"l2\"], dict_cat[\"l3\"], dict_cat[\"l4\"], dict_cat[\"l5\"], dict_cat[\"l6\"], product, name, piece_quantity, mrp, discount_max, discount_min, discount_flag, url])\n",
    "#             print(len(list_mandatory))\n",
    "            \n",
    "    #now since we got two things, first is the mandatory list and second is the optional list,\n",
    "    #we can get the dataset from both of them\n",
    "    get_dataset(list_mandatory, list_optional, set_with_columns, index)\n",
    "    #what this function does is\n",
    "    \n",
    "\n",
    "    return product_with_inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19839fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_having_all_categories=\"https://www.industrybuying.com/categories/\"\n",
    "\n",
    "urls, error=get_the_category_urls(url_having_all_categories)\n",
    "\n",
    "if error==1:\n",
    "    print(\"There is problem in the url having the categories\")\n",
    "    sys.exit()\n",
    "\n",
    "# urls=['https://www.industrybuying.com/agriculture-garden-landscaping-2384/', 'unknown', 'https://www.industrybuying.com/pumps-1160/', 'https://www.industrybuying.com/solar-4050/', 'please go away', 'https://www.industrybuying.com/welding-552/']\n",
    "    \n",
    "for index, i_th_url in enumerate(urls):\n",
    "    #implement this function and get all the url\n",
    "    list_subcategories, logs, error=find_all_urls(i_th_url)\n",
    "    \n",
    "    print(f\"The structure of the urls looks like:- (urls-{i_th_url})\")\n",
    "    for lis in list_subcategories:\n",
    "        print(len(lis), end=\" \")\n",
    "    print()\n",
    "    \n",
    "    #we have got the logs and error\n",
    "    #if the error ==1, we can simply return the dataframe having the heading as failed \n",
    "    if error==1:\n",
    "        data_logs = {'Status': ['Failed to retrieve category']}\n",
    "        df = pd.DataFrame(data_logs)\n",
    "        df.to_csv(f\"logs{index+1}.csv\", index=False)\n",
    "        #since the data_logs has no value in it, we can dont have the need to define the mandatory, optional and inconsistent urls\n",
    "        continue\n",
    "    #we must define the logs\n",
    "    #since we got the logs, we can simply find the dataframe\n",
    "    logs_table(logs, index+1)\n",
    "    \n",
    "    \n",
    "    product_with_inconsistency=data_collect(list_subcategories, index+1)\n",
    "    df = pd.DataFrame(product_with_inconsistency, columns=['URL', 'ISSUE'])\n",
    "    df.to_csv(f\"Inconsistent Product{index+1}.csv\", index=False)\n",
    "        \n",
    "    #now all we need to do is get the links which have inconsistency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6a4de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
